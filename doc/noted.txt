Learn Kubernetes using Interactive Browser-Based Scenarios
1. Launch A Single Node Cluster
Minikube is a tool that makes it easy to run Kubernetes locally. 
Minikube runs a single-node Kubernetes cluster inside a VM on your 
laptop for users looking to try out Kubernetes or develop with it day-to-day.
STEP1:
Minikube has been installed and configured in the environment. 
Check that it is properly installed, by running the minikube version command:
		minikube version
Start the cluster, by running the minikube start command:
		minikube start --wait=false
Great! You now have a running Kubernetes cluster in your online terminal. 
Minikube started a virtual machine for you, and a Kubernetes cluster is now 
running in that VM.
STEP2:
The cluster can be interacted with using the kubectl CLI. 
This is the main approach used for managing Kubernetes and the applications 
running on top of the cluster.Details of the cluster and its health status can 
be discovered via 
		kubectl cluster-info
To view the nodes in the cluster using 
		kubectl get nodes
If the node is marked as NotReady then it is still starting the components. This 
command shows all nodes that can be used to host our applications. Now we have 
only one node, and we can see that itâ€™s status is ready (it is ready to accept 
applications for deployment).
STEP3:
With a running Kubernetes cluster, containers can now be deployed.Using kubectl run, 
it allows containers to be deployed onto the cluster 
		- kubectl create deployment first-deployment --image=katacoda/docker
		-http-server
The status of the deployment can be discovered via the running Pods 
		- kubectl get pods
Once the container is running it can be exposed via different networking options, 
depending on requirements. One possible solution is NodePort, that provides a dynamic 
port to a container.
		kubectl expose deployment first-deployment --port=80 --type=NodePort
The command below finds the allocated port and executes a HTTP request.

		export PORT=$(kubectl get svc first-
		deployment -o go-
		template='{{range.spec.ports}}{{if 
		.nodePort}}{{.nodePort}}{{"\n"}}{{end}}{
		{end}}')
		echo "Accessing host01:$PORT"
		curl host01:$PORT
The result is the container that processed the request.
STEP4:
Enable the dashboard using Minikube with the command 
		minikube addons enable dashboard
Make the Kubernetes Dashboard available by deploying the following YAML definition. 
This should only be used on Katacoda.
		kubectl apply -f /opt/kubernetes-dashboard.yaml
The Kubernetes dashboard allows you to view your applications in a UI. In this deployment, 
the dashboard has been made available on port 30000 but may take a while to start.To see the
progress of the Dashboard starting, watch the Pods within the kube-system namespace using 
		kubectl get pods -n kubernetes-dashboard -w
Once running, the URL to the dashboard is 
		https://2886795321-30000-ollie09.environments.katacoda.com/

2. Launch a multi-node cluster using Kubeadm
STEP1:
Kubeadm has been installed on the nodes. Packages are available for Ubuntu 16.04+, 
CentOS 7 or HypriotOS v1.0.1+.The first stage of initialising the cluster is to launch the 
master node. The master is responsible for running the control plane components, etcd and 
the API server. Clients will communicate to the API to schedule workloads and manage the 
state of the cluster.
Task
The command below will initialise the cluster with a known token to simplify the following 
steps.
		kubeadm init --token=102952.1a7dd4cc8d1f4cc5 --kubernetes-version 
		$(kubeadm version -o short)
In production, it's recommend to exclude the token causing kubeadm to generate one on your 
behalf.To manage the Kubernetes cluster, the client configuration and certificates are 
required. This configuration is created when kubeadm initialises the cluster. The command 
copies the configuration to the users home directory and sets the environment variable for 
use with the CLI.
		sudo cp /etc/kubernetes/admin.conf $HOME/
		sudo chown $(id -u):$(id -g) $HOME/admin.conf
		export KUBECONFIG=$HOME/admin.conf
STEP2:
The Container Network Interface (CNI) defines how the different nodes and their workloads 
should communicate. There are multiple network providers available, some are listed here.
Task
In this scenario we'll use WeaveWorks. The deployment definition can be viewed at 
		cat /opt/weave-kube.yaml
This can be deployed using kubectl apply.
		kubectl apply -f /opt/weave-kube.yaml
Weave will now deploy as a series of Pods on the cluster. The status of this can be viewed 
using the command 
		kubectl get pod -n kube-system
When installing Weave on your cluster, visit 
		https://www.weave.works/docs/net/latest/kube-addon/ 
STEP3:
Once the Master and CNI has initialised, additional nodes can join the cluster as long as 
they have the correct token. The tokens can be managed via kubeadm token, for example 
		kubeadm token list.
Task
On the second node, run the command to join the cluster providing the IP address of the 
Master node.
		kubeadm join --discovery-token-unsafe-skip-ca-verification --token=102952.1a
		7dd4cc8d1f4cc5 172.17.0.18:6443
This is the same command provided after the Master has been initialised.The --discovery-token
-unsafe-skip-ca-verification tag is used to bypass the Discovery Token verification. As this 
token is generated dynamically, we couldn't include it within the steps. When in production, 
use the token provided by kubeadm init.
STEP4:
The cluster has now been initialised. The Master node will manage the cluster, while our 
one worker node will run our container workloads.
Task
The Kubernetes CLI, known as kubectl, can now use the configuration to access the cluster. 
For example, the command below will return the two nodes in our cluster.
		kubectl get nodes
STEP5:
The state of the two nodes in the cluster should now be Ready. This means that our deployments 
can be scheduled and launched.Using Kubectl, it's possible to deploy pods. Commands are always 
issued for the Master with each node only responsible for executing the workloads.The command 
below create a Pod based on the Docker Image katacoda/docker-http-server.
		kubectl create deployment http --image=katacoda/docker-http-server:latest
The status of the Pod creation can be viewed using 
		kubectl get pods
Once running, you can see the Docker Container running on the node.
		docker ps | grep docker-http-server
STEP6:
Kubernetes has a web-based dashboard UI giving visibility into the Kubernetes cluster.
Task
Deploy the dashboard yaml with the command 
		kubectl apply -f dashboard.yaml
The dashboard is deployed into the kube-system namespace. View the status of the deployment with 
		kubectl get pods -n kube-system
A ServiceAccount is required to login. A ClusterRoleBinding is used to assign the new ServiceAccount 
(admin-user) the role of cluster-admin on the cluster.
		cat <<EOF | kubectl create -f - 
		apiVersion: v1
		kind: ServiceAccount
		metadata:
		name: admin-user
		namespace: kube-system
		---
		apiVersion: rbac.authorization.k8s.io/v1beta1
		kind: ClusterRoleBinding
		metadata:
		name: admin-user
		roleRef:
		apiGroup: rbac.authorization.k8s.io
		kind: ClusterRole
		name: cluster-admin
		subjects:
		- kind: ServiceAccount
		name: admin-user
		namespace: kube-system
		EOF
This means they can control all aspects of Kubernetes. With ClusterRoleBinding and RBAC, different
level of permissions can be defined based on security requirements. More information on creating 
a user for the Dashboard can be found in the Dashboard documentation.
Once the ServiceAccount has been created, the token to login can be found with:
		kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | 
		grep admin-user | awk '{print $1}')
When the dashboard was deployed, it used externalIPs to bind the service to port 8443. This makes 
the dashboard available to outside of the cluster and viewable at https://2886795288-8443-ollie07.
environments.katacoda.com/Use the admin-user token to access the dashboard.For production, 
instead of externalIPs, it's recommended to use kubectl proxy to access the dashboard. See more 
details at https://github.com/kubernetes/dashboard.
3.Deploy Containers Using Kubectl
STEP1:
To start we need to launch a Kubernetes cluster.Execute the command below to start the cluster 
components and download the Kubectl CLI.
		minikube start --wait=false
Wait for the Node to become Ready by checking 
		kubectl get nodes
STEP2:
The run command creates a deployment based on the parameters specified, such as the image or 
replicas. This deployment is issued to the Kubernetes master which launches the Pods and containers
required. Kubectl run_ is similar to docker run but at a cluster level.The format of the command 
is kubectl run <name of deployment> <properties>
Task
The following command will launch a deployment called http which will start a container based on 
the Docker Image katacoda/docker-http-server:latest.
		kubectl run http --image=katacoda/docker-http-server:latest --replicas=1
You can then use kubectl to view the status of the deployments
		kubectl get deployments
To find out what Kubernetes created you can describe the deployment process.
		kubectl describe deployment http
The description includes how many replicas are available, labels specified and the events associate
with the deployment. These events will highlight any problems and errors that might have occurred.
In the next step we'll expose the running service.
STEP3:
With the deployment created, we can use kubectl to create a service which exposes the Pods on a 
particular port.Expose the newly deployed http deployment via kubectl expose. The command allows 
you to define the different parameters of the service and how to expose the deployment.
Task
Use the following command to expose the container port 80 on the host 8000 binding to the external-
ip of the host.
		kubectl expose deployment http --external-ip="172.17.0.27" --port=8000 --target-
		port=80
You will then be able to ping the host and see the result from the HTTP service.
		curl http://172.17.0.27:8000
STEP4:
With kubectl run it's possible to create the deployment and expose it as a single command.
Task
Use the command command to create a second http service exposed on port 8001.
		kubectl run httpexposed --image=katacoda/docker-http-server:latest --replicas=1 
		--port=80 --hostport=8001
You should be able to access it using curl http://172.17.0.27:8001 Under the covers, this exposes 
the Pod via Docker Port Mapping. As a result, you will not see the service listed using 
		kubectl get svc
To find the details you can use 
		docker ps | grep httpexposed
Pause Containers
Running the above command you'll notice the ports are exposed on the Pod, not the http container 
itself. The Pause container is responsible for defining the network for the Pod. Other containers 
in the pod share the same network namespace. This improves network performance and allow multiple 
containers to communicate over the same network interface..
STEP6:
With our deployment running we can now use kubectl to scale the number of replicas.Scaling the 
deployment will request Kubernetes to launch additional Pods. These Pods will then automatically 
be load balanced using the exposed Service.
Task
The command kubectl scale allows us to adjust the number of Pods running for a particular deployment 
or replication controller.
		kubectl scale --replicas=3 deployment http
Listing all the pods, you should see three running for the http deployment 
		kubectl get pods
Once each Pod starts it will be added to the load balancer service. By describing the service you 
can view the endpoint and the associated Pods which are included.
		kubectl describe svc http
Making requests to the service will request in different nodes processing the request.
		curl http://172.17.0.27:8000
4: Deploy Containers Using YAML 














